[<<< Previous](power.md) | [Next >>>](accessibility.md)

# Level of Impact III: Social, Political, and Economic Impacts of Projects or Research  

## Social, political, and economic impacts

> "At a third level of impact, we can consider **the social, economic, or political changes caused** by one’s research processes or products, in both the short and long term." ([Annette Markham, "OKCupid data release fiasco: It’s time to rethink ethics education," 2016](http://annettemarkham.com/2016/05/okcupid-data-release-fiasco-its-time-to-rethink-ethics-education/), emphasis added)  

Some questions to consider:

- Whose **labor** and what **materials** are used to make the digital tools you use? How should we (those who benefit from the labor of other people) attribute others' labor? How can we (users of these tools) be held accountable?  

> "the energy demands for digital daily life, a key source of big data for social science research, are significant in this era of climate change ... should researchers take the lead in asking cloud storage providers and data processing centers to shift to sustainable and renewable energy sources?" ([Matthew Zook et al, "Ten simple rules for responsible big data research," 2017](http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005399))  

* Could your research or project be used to justify or facilitate potentially harmful **control** or **surveillance**?    

> "'Lots of proprietary programs spy. Windows spies, Mac OS spies, iOS spies, Flash player spies. Thousands of apps spy on the user' ... The internet of things—well I call it the internet of stings—it's a way that those companies can get power over more things in your life, snoop on more things in your life and have total power... 'Portable phones, every portable phone has a universal backdoor… And with the backdoor [the phone company] can convert it into a full-time listening device that can hear all the conversation in the room, even when it's not making any call, even when it’s supposed to be switched off.'" ([Factor, interview with Richard Stallman, "The Vanishing State of Privacy," 2017](http://magazine.factor-tech.com/factor_winter_2017/richard_stallman_and_the_vanishing_state_of_privacy))  

> [Even more so now] "In a post 9/11 world, the U. S. government utilizes computer technology to exert some degree of control over its citizens, rather than protect their privacy... [Also,] internet software can be used as parasocietal mechanisms for the observation of online interactions. Online social networks allow for high levels of surveillance. In addition to marketers, college officials and parents can access social networking sites. Students may think that their Facebook or MySpace journal entries are private but they are actually public diaries." ([Susan Barnes, “A Privacy Paradox,” 2006](http://firstmonday.org/article/view/1394/1312))   

> "Other cases show that 'public' datasets are easily adapted for highly invasive research by incorporating other data, such as Hague et al.’s use of property records and geographic profiling techniques to allegedly identify the pseudonymous artist Banksy" ([Matthew Zook et al, "Ten simple rules for responsible big data research," 2017](http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005399))  

* Could it influence **social or political discourse**? Modes of **profit**?

> "to what extent is a research project focused on enhancing the public good or the underserved of society? Are questions about equity or promoting other public values being addressed in one’s data streams, or is a big data focus rendering them invisible or irrelevant to your analysis [37]? How can increasingly vulnerable yet fundamentally important public resources—such as state-mandated cancer registries—be protected? How might research aid or inhibit different business and political actors?" ([Matthew Zook, et al, "Ten simple rules for responsible big data research," 2017](http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005399))  

These are *big* questions.  

A researcher should not be expected to have all the answers or predict the future. The aim here is to thoughtfully consider these questions, to think critically about potential positive and negative effects of research and projects, and to be responsible and accountable.  

As Annette Markham reminds us, as discussed earlier in this session:  

> "an impact approach is targeted toward the possible or probable impact, rather than the prevention of impact in the first place. It acknowledges that we change the world as we conduct even the smallest of scientific studies, and therefore, we must take some personal responsibility for our methods." ([Annette Markham, "OKCupid data release fiasco: It’s time to rethink ethics education," 2016](http://annettemarkham.com/2016/05/okcupid-data-release-fiasco-its-time-to-rethink-ethics-education/))  

![close-up photo of a drop of water falling into a pool of water, creating a series of concentric ripples](../images/ripple.jpg)  
Image source: [Sergiu Bacioiu from Romania, "Ripple effect on water," Wikimedia, Creative Commons Attribution 2.0 Generic license](https://commons.wikimedia.org/wiki/File:Ripple_effect_on_water.jpg)  

Further readings:  
[Xiang Biao, *Global Body Shopping: An Indian Labor System in the Information Technology Industry*, 2007](https://press.princeton.edu/titles/8315.html)  
[Elsa Davidson, *The Burdens of Aspiration: Schools, Youth, and Success in the Divided Social Worlds of Silicon Valley*, 2011](https://nyupress.org/books/9780814720875/)  
[Anna Lauren Hoffmann, "Data Violence and How Bad Engineering Choices Can Damage Society," 2018](https://medium.com/s/story/data-violence-and-how-bad-engineering-choices-can-damage-society-39e44150e1d4)  
[The Policing in Chicago Research Group, "Tracked and Targeted: Early Findings on Chicago's Gang Database," 2018](http://erasethedatabase.com/wp-content/uploads/2018/02/Tracked-Targeted-0217.pdf)  
[Safiya Umoja Noble, *Algorithms of Oppression: How Search Engines Reinforce Racism*, 2018](https://nyupress.org/books/9781479837243/)  
[Cathy O'Neil, *Methods of Math Destruction: How Big Data Increases Inequality and Threatens Democracy*, 2016](https://weaponsofmathdestructionbook.com/)  
[Matthew Zook et al, "Ten simple rules for responsible big data research," 2017](http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005399)  

## Activity   

Think about the digital project or research you are or will be working on. Pair up with another person near you and discuss:  

* Whose labor and what materials do you rely upon to do your work?  
* Could your research or project be used to justify or facilitate potentially harmful control or surveillance-by e.g. the state, a vigilante group, an abusive partner?  
* How could your work cause changes to or justify social, economic or political discourses?  
* Will your work be used for profit, for who?  

Share as a class.    

# Data is people!
* "Each of us has a “data double,” a digital duplicate of our lives captured in data and spread across assemblages of information systems."](https://datadoubles.org/2018/05/01/what-is-a-data-double/)
* [Data doubles project](https://datadoubles.org/project/)

## Thinking in terms of "levels of impact"

An "impact approach" foregrounds:

> "... the possible or probable impact, rather than the prevention of impact in the first place. It acknowledges that we change the world as we conduct even the smallest of scientific studies, and therefore, we must take some personal responsibility for our methods." ([Annette Markham, "OKCupid data release fiasco: It’s time to rethink ethics education," 2016](http://annettemarkham.com/2016/05/okcupid-data-release-fiasco-its-time-to-rethink-ethics-education/))

> Drawing from Markham (2016), let's focus on three levels of impact:  
1. Direct impacts on people
2. Ramifications of (re)producing categories
3. Social, political and economic effects  

Additionally, this workshop will address the range of impact, or the range of accessibility to your work:  

- to people with disabilities,
- to people in different countries or who speak different languages, and
- in terms of cost and proprietary accessibility.

Source: [Annette Markham, "OKCupid data release fiasco: It’s time to rethink ethics education," 2016](http://annettemarkham.com/2016/05/okcupid-data-release-fiasco-its-time-to-rethink-ethics-education/)    


## Social Media

* Accessing social media data can be a complicated mix of licensing, permission and ethics. 

## Example: Facebook 

* "Smarting from the Cambridge Analytica debacle in 2018, Facebook promised a research initiative to give academics access to its data. I lead one of a dozen research groups that were granted access in February to a large data set, which is still less than was promised, after more than a year of delays. Our project aimed to determine whether disinformation campaigns have a measurable effect on polls. But the data provided are nearly useless for answering this and many other research questions, and are far inferior to what Facebook gives private companies"
[Facebook needs to share more with researchers](https://www.nature.com/articles/d41586-020-00828-5)
[Additional context](https://www.axios.com/facebook-researchers-data-social-science-one-62dc5018-21d7-49f6-8149-ff421e611dfc.html)

* "It was an episode that frustrated employees who wanted to reduce racial bias on the platform but one that they said did not surprise them. Facebook management has repeatedly ignored and suppressed internal research showing racial bias in the way that the platform removes content, according to eight current and former employees, all of whom requested anonymity to discuss internal Facebook business." 
(https://www.nbcnews.com/tech/tech-news/facebook-management-ignored-internal-research-showing-racial-bias-current-former-n1234746)

[<<< Previous](power.md) | [Next >>>](accessibility.md)
-----
[Return to introduction](https://github.com/SouthernMethodistUniversity/access)
